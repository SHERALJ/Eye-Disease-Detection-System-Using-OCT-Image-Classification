{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ae52ae",
   "metadata": {},
   "source": [
    "# DEEP LEANINNG MODEL - ConvNext-TINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0461547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 0: Imports, GPU check, path helper, seeds ===\n",
    "import os, sys, time, json, math, platform, random, csv\n",
    "from pathlib import PureWindowsPath\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "def is_wsl():\n",
    "    try:\n",
    "        return (\"microsoft\" in platform.release().lower()) or (\"wsl\" in platform.version().lower())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def win_to_wsl_path(win_path: str) -> str:\n",
    "    if not is_wsl(): return win_path\n",
    "    if \":\" not in win_path: return win_path\n",
    "    p = PureWindowsPath(win_path)\n",
    "    drive = str(p.drive).replace(\":\", \"\").lower()\n",
    "    tail = str(p).replace(\"\\\\\", \"/\").split(\":/\")[-1]\n",
    "    return f\"/mnt/{drive}/{tail}\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # keep False for speed\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d8d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /mnt/c/Users/sheno/OneDrive/CODCSD201F-006-SetupFile/Desktop/FINAL/dataset/OCT2017_70_15_15\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Config ===\n",
    "# >>> UPDATE THIS PATH IF NEEDED <<<\n",
    "BASE_DIR = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_70_15_15\"\n",
    "BASE_DIR = win_to_wsl_path(BASE_DIR)\n",
    "\n",
    "OUT_DIR        = \"convnext_tiny_run\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CLASSES        = [\"CNV\",\"DME\",\"DRUSEN\",\"NORMAL\"]\n",
    "NUM_CLASSES    = len(CLASSES)\n",
    "IMG_SIZE       = 224         # ConvNeXt is trained on 224; you can try 128 for speed\n",
    "BATCH_SIZE     = 32\n",
    "NUM_WORKERS    = 4           # tune per your CPU\n",
    "EPOCHS         = 15\n",
    "LR             = 5e-4\n",
    "WEIGHT_DECAY   = 0.05\n",
    "LABEL_SMOOTH   = 0.1\n",
    "PATIENCE       = 5           # early stopping\n",
    "\n",
    "MODEL_NAME     = \"convnext_tiny\"\n",
    "CHECKPOINT_BEST = os.path.join(OUT_DIR, f\"{MODEL_NAME}_best.pt\")\n",
    "METRICS_CSV     = os.path.join(OUT_DIR, f\"{MODEL_NAME}_metrics.csv\")\n",
    "CONF_JSON       = os.path.join(OUT_DIR, f\"{MODEL_NAME}_confusion_matrix.json\")\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8ca30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Ensure timm is available ===\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    !pip -q install timm\n",
    "    import timm\n",
    "\n",
    "# ImageNet normalization for ConvNeXt\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),     # OCT often grayscale -> 3-ch\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(int(IMG_SIZE*1.14)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7faaae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCAN] train: /mnt/c/Users/sheno/OneDrive/CODCSD201F-006-SetupFile/Desktop/FINAL/dataset/OCT2017_70_15_15/train -> classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
      "[SCAN] val: /mnt/c/Users/sheno/OneDrive/CODCSD201F-006-SetupFile/Desktop/FINAL/dataset/OCT2017_70_15_15/val -> classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
      "[SCAN] test: /mnt/c/Users/sheno/OneDrive/CODCSD201F-006-SetupFile/Desktop/FINAL/dataset/OCT2017_70_15_15/test -> classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
      "[INFO] Class->index mapping: {'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
      "[INFO] Train counts: {'CNV': 26216, 'DME': 8116, 'DRUSEN': 6201, 'NORMAL': 18593}\n",
      "[INFO] Val counts: {'CNV': 5618, 'DME': 1739, 'DRUSEN': 1329, 'NORMAL': 3984}\n",
      "[INFO] Test counts: {'CNV': 5617, 'DME': 1739, 'DRUSEN': 1329, 'NORMAL': 3984}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Datasets & Loaders ===\n",
    "train_dir = os.path.join(BASE_DIR, \"train\")\n",
    "val_dir   = os.path.join(BASE_DIR, \"val\")\n",
    "test_dir  = os.path.join(BASE_DIR, \"test\")\n",
    "\n",
    "for split in (\"train\",\"val\",\"test\"):\n",
    "    sd = os.path.join(BASE_DIR, split)\n",
    "    present = [c for c in CLASSES if os.path.isdir(os.path.join(sd, c))]\n",
    "    print(f\"[SCAN] {split}: {sd} -> classes: {present}\")\n",
    "\n",
    "ds_train = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "ds_val   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
    "ds_test  = datasets.ImageFolder(test_dir,  transform=val_tfms)\n",
    "\n",
    "print(\"[INFO] Class->index mapping:\", ds_train.class_to_idx)\n",
    "\n",
    "def count_per_class(ds):\n",
    "    counts = Counter([y for _, y in ds.samples])\n",
    "    inv = {v:k for k,v in ds.class_to_idx.items()}\n",
    "    return {inv[k]: counts.get(k,0) for k in sorted(counts)}\n",
    "\n",
    "print(\"[INFO] Train counts:\", count_per_class(ds_train))\n",
    "print(\"[INFO] Val counts:\",   count_per_class(ds_val))\n",
    "print(\"[INFO] Test counts:\",  count_per_class(ds_test))\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "loader_test  = DataLoader(ds_test,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24df30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85930/3839834633.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Model, Loss, Optim, Sched ===\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES, drop_path_rate=0.1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Label smoothing improves calibration\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "print(model.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Utilities ===\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def top1_accuracy(logits, targets):\n",
    "    with torch.no_grad():\n",
    "        preds = logits.argmax(dim=1)\n",
    "        return (preds == targets).float().mean().item()\n",
    "\n",
    "def run_epoch(model, loader, train=True, max_norm=5.0):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        acc = top1_accuracy(logits, labels)\n",
    "        bs = imgs.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += acc * bs\n",
    "        n += bs\n",
    "        pbar.set_description(f\"{'Train' if train else 'Val'} loss {total_loss/n:.4f} | acc {total_acc/n:.4f}\")\n",
    "    return total_loss/n, total_acc/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b2eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/15 | train_loss 1.2778 acc 0.4388 | val_loss 1.2642 acc 0.4434 | lr 4.95e-04\n",
      "[SAVED] New best: val_acc=0.4434 -> convnext_tiny_run/convnext_tiny_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/15 | train_loss 1.2656 acc 0.4433 | val_loss 1.2635 acc 0.4434 | lr 4.78e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/15 | train_loss 1.2654 acc 0.4433 | val_loss 1.2659 acc 0.4434 | lr 4.52e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/15 | train_loss 1.2649 acc 0.4434 | val_loss 1.2645 acc 0.4434 | lr 4.17e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/15 | train_loss 1.2647 acc 0.4431 | val_loss 1.2645 acc 0.4434 | lr 3.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/15 | train_loss 1.2646 acc 0.4431 | val_loss 1.2649 acc 0.4434 | lr 3.27e-04\n",
      "[EARLY STOP] Patience 5 reached at epoch 6.\n",
      "[TIMER] Training finished in 704.5 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Training loop ===\n",
    "best_val_acc = 0.0\n",
    "pat_count = 0\n",
    "\n",
    "# CSV header\n",
    "if not os.path.isfile(METRICS_CSV):\n",
    "    with open(METRICS_CSV, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"epoch\",\"train_loss\",\"train_acc\",\"val_loss\",\"val_acc\",\"lr\"])\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = run_epoch(model, loader_train, train=True)\n",
    "    val_loss,   val_acc   = run_epoch(model, loader_val,   train=False)\n",
    "\n",
    "    # Step scheduler AFTER epoch\n",
    "    scheduler.step()\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"val_loss {val_loss:.4f} acc {val_acc:.4f} | lr {lr_now:.2e}\")\n",
    "\n",
    "    # log CSV\n",
    "    with open(METRICS_CSV, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f); w.writerow([epoch, f\"{train_loss:.6f}\", f\"{train_acc:.6f}\",\n",
    "                                       f\"{val_loss:.6f}\", f\"{val_acc:.6f}\", f\"{lr_now:.8f}\"])\n",
    "\n",
    "    # early stopping + checkpoint\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        pat_count = 0\n",
    "        torch.save({\"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"val_acc\": best_val_acc,\n",
    "                    \"classes\": CLASSES,\n",
    "                    \"img_size\": IMG_SIZE}, CHECKPOINT_BEST)\n",
    "        print(f\"[SAVED] New best: val_acc={best_val_acc:.4f} -> {CHECKPOINT_BEST}\")\n",
    "    else:\n",
    "        pat_count += 1\n",
    "        if pat_count >= PATIENCE:\n",
    "            print(f\"[EARLY STOP] Patience {PATIENCE} reached at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(f\"[TIMER] Training finished in {total_time/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bbe9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheno/tf/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Test Accuracy: 0.4434\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         CNV     0.4434    1.0000    0.6143      5617\n",
      "         DME     0.0000    0.0000    0.0000      1739\n",
      "      DRUSEN     0.0000    0.0000    0.0000      1329\n",
      "      NORMAL     0.0000    0.0000    0.0000      3984\n",
      "\n",
      "    accuracy                         0.4434     12669\n",
      "   macro avg     0.1108    0.2500    0.1536     12669\n",
      "weighted avg     0.1966    0.4434    0.2724     12669\n",
      "\n",
      "Confusion matrix:\n",
      " [[5617    0    0    0]\n",
      " [1739    0    0    0]\n",
      " [1329    0    0    0]\n",
      " [3984    0    0    0]]\n",
      "[SAVED] convnext_tiny_run/convnext_tiny_confusion_matrix.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheno/tf/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/sheno/tf/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Evaluation on test set ===\n",
    "# Load best\n",
    "ckpt = torch.load(CHECKPOINT_BEST, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "    for imgs, labels in tqdm(loader_test, desc=\"Testing\", leave=False):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.numpy())\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "acc = (all_preds == all_labels).mean()\n",
    "print(f\"[RESULT] Test Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(all_labels, all_preds, target_names=CLASSES, digits=4))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# save confusion matrix JSON\n",
    "with open(CONF_JSON, \"w\") as f:\n",
    "    json.dump(cm.tolist(), f, indent=2)\n",
    "print(f\"[SAVED] {CONF_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75221ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Inference helper ===\n",
    "from PIL import Image\n",
    "\n",
    "infer_tfms = val_tfms  # same as validation\n",
    "\n",
    "idx_to_class = {v:k for k,v in ds_train.class_to_idx.items()}\n",
    "\n",
    "def predict_image(path: str):\n",
    "    path = win_to_wsl_path(path)\n",
    "    img = Image.open(path).convert(\"L\")  # grayscale load\n",
    "    img = transforms.functional.to_pil_image(np.array(img))\n",
    "    img = infer_tfms(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "        logits = model(img)\n",
    "        prob = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "        pred_idx = int(np.argmax(prob))\n",
    "    return idx_to_class[pred_idx], {idx_to_class[i]: float(prob[i]) for i in range(len(prob))}\n",
    "\n",
    "# Example:\n",
    "# predict_image(r\"C:\\path\\to\\one\\OCT\\image.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f461a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] Final model -> convnext_tiny_run/convnext_tiny_final.pt\n"
     ]
    }
   ],
   "source": [
    "# === Save Final Model ===\n",
    "FINAL_MODEL_PATH = os.path.join(OUT_DIR, f\"{MODEL_NAME}_final.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"epoch\": epoch,\n",
    "    \"classes\": CLASSES,\n",
    "    \"img_size\": IMG_SIZE\n",
    "}, FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"[SAVED] Final model -> {FINAL_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5343e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference ✅\n"
     ]
    }
   ],
   "source": [
    "# === Load Final Model ===\n",
    "ckpt = torch.load(FINAL_MODEL_PATH, map_location=device)\n",
    "\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=len(ckpt[\"classes\"]))\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded and ready for inference ✅\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
