{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c02ebd",
   "metadata": {},
   "source": [
    "#  DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f063d",
   "metadata": {},
   "source": [
    "#  Setup Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Libraries & Packages (Imports in One Cell)\n",
    "# ============================================\n",
    "\n",
    "# --- System & Utilities\n",
    "import os, sys, glob, time, random, shutil, platform, warnings, json, csv, pickle\n",
    "from pathlib import Path, PureWindowsPath\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Numerical & Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# --- Deep Learning (TensorFlow / Keras)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, \n",
    "                                     GlobalAveragePooling2D, Dense, Dropout)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping, \n",
    "                                        ReduceLROnPlateau)\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "# --- Machine Learning (Scikit-learn)\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_curve, auc, f1_score, accuracy_score)\n",
    "\n",
    "# ============================================\n",
    "print(\"[INFO] All libraries successfully imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e3291",
   "metadata": {},
   "source": [
    "# GPU Setup (TensorFlow)\n",
    "We enable *memory growth* on all visible GPUs. This avoids TensorFlow pre-allocating all VRAM and prevents OOM issues when other processes (or notebooks) share the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83434b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] No GPU detected; running on CPU\n"
     ]
    }
   ],
   "source": [
    "# ========== 1) GPU: safe memory growth ==========\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"[GPU] Using {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"[GPU] No GPU detected; running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504777e1",
   "metadata": {},
   "source": [
    "# Dataset Root & Split Discovery\n",
    "This cell sets the dataset root and robustly resolves paths (Windows/WSL/OneDrive).\n",
    "It then detects the `train/`, `val/`, and `test/` folders and verifies that all\n",
    "expected OCT classes (`CNV`, `DME`, `DRUSEN`, `NORMAL`) are present.\n",
    "\n",
    "If the data lives directly under the root as class folders (no `train/val/test`),\n",
    "the code falls back to treating the root as the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] train: train -> C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\\train\n",
      "[DATA]  val : val -> C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\\val\n",
      "[DATA] test : test -> C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\\test\n",
      "[OK] Classes at C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\\train: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
      "[OK] Classes at C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\\test: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "# ========== 2) Data paths (auto-detect common names) ==========\n",
    "data_dir = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\"  # <--- change if needed\n",
    "\n",
    "# Expected classes (define early because later logic checks these)\n",
    "categories = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]\n",
    "expected = set(categories)\n",
    "\n",
    "# If the chosen data_dir doesn't exist (WSL vs Windows path mismatch), try fallbacks:\n",
    "if not os.path.isdir(data_dir):\n",
    "    print(f\"[WARN] data_dir not found: {data_dir}\")\n",
    "\n",
    "    # 1) Try known notebook variable INPUT_ROOT (Windows style) if available\n",
    "    try:\n",
    "        if 'INPUT_ROOT' in globals() and INPUT_ROOT and os.path.isdir(str(INPUT_ROOT)):\n",
    "            data_dir = str(INPUT_ROOT)\n",
    "            print(f\"[INFO] Falling back to INPUT_ROOT: {data_dir}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try converting /mnt/<drive>/... -> C:\\... (common WSL<->Windows mismatch)\n",
    "    if not os.path.isdir(data_dir):\n",
    "        try:\n",
    "            p = Path(data_dir)\n",
    "            parts = p.parts\n",
    "            # expected form: ('/', 'mnt', 'c', 'Users', ...)\n",
    "            if len(parts) > 2 and parts[0] == '/' and parts[1] == 'mnt':\n",
    "                drive = parts[2]\n",
    "                if len(drive) == 1:\n",
    "                    drive_letter = f\"{drive.upper()}:\"\n",
    "                    win_path = Path(drive_letter, *parts[3:])\n",
    "                    if os.path.isdir(str(win_path)):\n",
    "                        data_dir = str(win_path)\n",
    "                        print(f\"[INFO] Found Windows-equivalent path: {data_dir}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Try a more permissive replacement of the prefix (covers some variants)\n",
    "    if not os.path.isdir(data_dir):\n",
    "        try:\n",
    "            s = data_dir\n",
    "            if s.startswith('/mnt/'):\n",
    "                # replace '/mnt/c/...' -> 'C:/...'\n",
    "                parts = s.split('/')\n",
    "                if len(parts) > 2 and len(parts[2]) == 1:\n",
    "                    drive_letter = parts[2].upper() + ':'\n",
    "                    rest = \"/\".join(parts[3:])\n",
    "                    alt = os.path.join(drive_letter, rest.replace('/', os.sep))\n",
    "                    if os.path.isdir(alt):\n",
    "                        data_dir = alt\n",
    "                        print(f\"[INFO] Found alternative Windows path: {data_dir}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find dataset root. Checked data_dir and fallbacks. \"\n",
    "        f\"Original data_dir: {data_dir}. \"\n",
    "        f\"Please set data_dir to the correct dataset root or adjust the path.\"\n",
    "    )\n",
    "\n",
    "def _pick_existing(root, candidates):\n",
    "    for name in candidates:\n",
    "        p = os.path.join(root, name)\n",
    "        if os.path.isdir(p):\n",
    "            return p, name\n",
    "    return None, None\n",
    "\n",
    "train_path, train_name = _pick_existing(data_dir, [\"train\",\"training\",\"Train\",\"TRAIN\"])\n",
    "val_path,   val_name   = _pick_existing(data_dir, [\"val\",\"validation\",\"valid\",\"Val\",\"Validation\"])\n",
    "test_path,  test_name  = _pick_existing(data_dir, [\"test\",\"testing\",\"Test\",\"Testing\"])\n",
    "\n",
    "# If train/test directories are not present directly under data_dir, it's possible\n",
    "# the dataset layout is class folders directly under data_dir (no split folders).\n",
    "# In that case treat data_dir as 'train' and leave test unset so later logic can use KFold.\n",
    "if not train_path and any(os.path.isdir(os.path.join(data_dir, d)) for d in os.listdir(data_dir)):\n",
    "    # check if data_dir itself contains class subfolders matching expected classes\n",
    "    subdirs = {d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))}\n",
    "    if expected.issubset(subdirs):\n",
    "        train_path, train_name = data_dir, os.path.basename(data_dir)\n",
    "        print(f\"[INFO] Using {data_dir} as training root (found class folders directly).\")\n",
    "\n",
    "if not train_path or not test_path:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find train/test under {data_dir}. \"\n",
    "        f\"Tried train/training and test/testing. If your dataset uses a single split (all images under class folders), set data_dir accordingly.\"\n",
    "    )\n",
    "\n",
    "print(f\"[DATA] train: {train_name} -> {train_path}\")\n",
    "print(f\"[DATA]  val : {val_name or 'NONE (KFold)'} -> {val_path}\")\n",
    "print(f\"[DATA] test : {test_name} -> {test_path}\")\n",
    "\n",
    "def _check_classes(path):\n",
    "    sub = {d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))}\n",
    "    missing = expected - sub\n",
    "    if missing:\n",
    "        print(f\"[WARN] {path} missing classes: {sorted(missing)}\")\n",
    "    else:\n",
    "        print(f\"[OK] Classes at {path}: {sorted(sub)}\")\n",
    "_check_classes(train_path)\n",
    "_check_classes(test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9cff4",
   "metadata": {},
   "source": [
    "# Preprocessing Step 1: Resize to 224×224 (Aspect-Ratio Preserved)\n",
    "We resize all images to **224×224** using *letterboxing* (black padding). This\n",
    "standardizes input sizes without cropping retinal tissue. The output mirrors the\n",
    "original folder structure and is written to `OCT2017_Resize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e386a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84484 images under C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\n",
      "Processed 100/84484\n",
      "Processed 200/84484\n",
      "Processed 300/84484\n",
      "Processed 400/84484\n",
      "Processed 500/84484\n",
      "Processed 600/84484\n",
      "Processed 700/84484\n",
      "Processed 800/84484\n",
      "Processed 900/84484\n",
      "Processed 1000/84484\n",
      "Processed 1100/84484\n",
      "Processed 1200/84484\n",
      "Processed 1300/84484\n",
      "Processed 1400/84484\n",
      "Processed 1500/84484\n",
      "Processed 1600/84484\n",
      "Processed 1700/84484\n",
      "Processed 1800/84484\n",
      "Processed 1900/84484\n",
      "Processed 2000/84484\n",
      "Processed 2100/84484\n",
      "Processed 2200/84484\n",
      "Processed 2300/84484\n",
      "Processed 2400/84484\n",
      "Processed 2500/84484\n",
      "Processed 2600/84484\n",
      "Processed 2700/84484\n",
      "Processed 2800/84484\n",
      "Processed 2900/84484\n",
      "Processed 3000/84484\n",
      "Processed 3100/84484\n",
      "Processed 3200/84484\n",
      "Processed 3300/84484\n",
      "Processed 3400/84484\n",
      "Processed 3500/84484\n",
      "Processed 3600/84484\n",
      "Processed 3700/84484\n",
      "Processed 3800/84484\n",
      "Processed 3900/84484\n",
      "Processed 4000/84484\n",
      "Processed 4100/84484\n",
      "Processed 4200/84484\n",
      "Processed 4300/84484\n",
      "Processed 4400/84484\n",
      "Processed 4500/84484\n",
      "Processed 4600/84484\n",
      "Processed 4700/84484\n",
      "Processed 4800/84484\n",
      "Processed 4900/84484\n",
      "Processed 5000/84484\n",
      "Processed 5100/84484\n",
      "Processed 5200/84484\n",
      "Processed 5300/84484\n",
      "Processed 5400/84484\n",
      "Processed 5500/84484\n",
      "Processed 5600/84484\n",
      "Processed 5700/84484\n",
      "Processed 5800/84484\n",
      "Processed 5900/84484\n",
      "Processed 6000/84484\n",
      "Processed 6100/84484\n",
      "Processed 6200/84484\n",
      "Processed 6300/84484\n",
      "Processed 6400/84484\n",
      "Processed 6500/84484\n",
      "Processed 6600/84484\n",
      "Processed 6700/84484\n",
      "Processed 6800/84484\n",
      "Processed 6900/84484\n",
      "Processed 7000/84484\n",
      "Processed 7100/84484\n",
      "Processed 7200/84484\n",
      "Processed 7300/84484\n",
      "Processed 7400/84484\n",
      "Processed 7500/84484\n",
      "Processed 7600/84484\n",
      "Processed 7700/84484\n",
      "Processed 7800/84484\n",
      "Processed 7900/84484\n",
      "Processed 8000/84484\n",
      "Processed 8100/84484\n",
      "Processed 8200/84484\n",
      "Processed 8300/84484\n",
      "Processed 8400/84484\n",
      "Processed 8500/84484\n",
      "Processed 8600/84484\n",
      "Processed 8700/84484\n",
      "Processed 8800/84484\n",
      "Processed 8900/84484\n",
      "Processed 9000/84484\n",
      "Processed 9100/84484\n",
      "Processed 9200/84484\n",
      "Processed 9300/84484\n",
      "Processed 9400/84484\n",
      "Processed 9500/84484\n",
      "Processed 9600/84484\n",
      "Processed 9700/84484\n",
      "Processed 9800/84484\n",
      "Processed 9900/84484\n",
      "Processed 10000/84484\n",
      "Processed 10100/84484\n",
      "Processed 10200/84484\n",
      "Processed 10300/84484\n",
      "Processed 10400/84484\n",
      "Processed 10500/84484\n",
      "Processed 10600/84484\n",
      "Processed 10700/84484\n",
      "Processed 10800/84484\n",
      "Processed 10900/84484\n",
      "Processed 11000/84484\n",
      "Processed 11100/84484\n",
      "Processed 11200/84484\n",
      "Processed 11300/84484\n",
      "Processed 11400/84484\n",
      "Processed 11500/84484\n",
      "Processed 11600/84484\n",
      "Processed 11700/84484\n",
      "Processed 11800/84484\n",
      "Processed 11900/84484\n",
      "Processed 12000/84484\n",
      "Processed 12100/84484\n",
      "Processed 12200/84484\n",
      "Processed 12300/84484\n",
      "Processed 12400/84484\n",
      "Processed 12500/84484\n",
      "Processed 12600/84484\n",
      "Processed 12700/84484\n",
      "Processed 12800/84484\n",
      "Processed 12900/84484\n",
      "Processed 13000/84484\n",
      "Processed 13100/84484\n",
      "Processed 13200/84484\n",
      "Processed 13300/84484\n",
      "Processed 13400/84484\n",
      "Processed 13500/84484\n",
      "Processed 13600/84484\n",
      "Processed 13700/84484\n",
      "Processed 13800/84484\n",
      "Processed 13900/84484\n",
      "Processed 14000/84484\n",
      "Processed 14100/84484\n",
      "Processed 14200/84484\n",
      "Processed 14300/84484\n",
      "Processed 14400/84484\n",
      "Processed 14500/84484\n",
      "Processed 14600/84484\n",
      "Processed 14700/84484\n",
      "Processed 14800/84484\n",
      "Processed 14900/84484\n",
      "Processed 15000/84484\n",
      "Processed 15100/84484\n",
      "Processed 15200/84484\n",
      "Processed 15300/84484\n",
      "Processed 15400/84484\n",
      "Processed 15500/84484\n",
      "Processed 15600/84484\n",
      "Processed 15700/84484\n",
      "Processed 15800/84484\n",
      "Processed 15900/84484\n",
      "Processed 16000/84484\n",
      "Processed 16100/84484\n",
      "Processed 16200/84484\n",
      "Processed 16300/84484\n",
      "Processed 16400/84484\n",
      "Processed 16500/84484\n",
      "Processed 16600/84484\n",
      "Processed 16700/84484\n",
      "Processed 16800/84484\n",
      "Processed 16900/84484\n",
      "Processed 17000/84484\n",
      "Processed 17100/84484\n",
      "Processed 17200/84484\n",
      "Processed 17300/84484\n",
      "Processed 17400/84484\n",
      "Processed 17500/84484\n",
      "Processed 17600/84484\n",
      "Processed 17700/84484\n",
      "Processed 17800/84484\n",
      "Processed 17900/84484\n",
      "Processed 18000/84484\n",
      "Processed 18100/84484\n",
      "Processed 18200/84484\n",
      "Processed 18300/84484\n",
      "Processed 18400/84484\n",
      "Processed 18500/84484\n",
      "Processed 18600/84484\n",
      "Processed 18700/84484\n",
      "Processed 18800/84484\n",
      "Processed 18900/84484\n",
      "Processed 19000/84484\n",
      "Processed 19100/84484\n",
      "Processed 19200/84484\n",
      "Processed 19300/84484\n",
      "Processed 19400/84484\n",
      "Processed 19500/84484\n",
      "Processed 19600/84484\n",
      "Processed 19700/84484\n",
      "Processed 19800/84484\n",
      "Processed 19900/84484\n",
      "Processed 20000/84484\n",
      "Processed 20100/84484\n",
      "Processed 20200/84484\n",
      "Processed 20300/84484\n",
      "Processed 20400/84484\n",
      "Processed 20500/84484\n",
      "Processed 20600/84484\n",
      "Processed 20700/84484\n",
      "Processed 20800/84484\n",
      "Processed 20900/84484\n",
      "Processed 21000/84484\n",
      "Processed 21100/84484\n",
      "Processed 21200/84484\n",
      "Processed 21300/84484\n",
      "Processed 21400/84484\n",
      "Processed 21500/84484\n",
      "Processed 21600/84484\n",
      "Processed 21700/84484\n",
      "Processed 21800/84484\n",
      "Processed 21900/84484\n",
      "Processed 22000/84484\n",
      "Processed 22100/84484\n",
      "Processed 22200/84484\n",
      "Processed 22300/84484\n",
      "Processed 22400/84484\n",
      "Processed 22500/84484\n",
      "Processed 22600/84484\n",
      "Processed 22700/84484\n",
      "Processed 22800/84484\n",
      "Processed 22900/84484\n",
      "Processed 23000/84484\n",
      "Processed 23100/84484\n",
      "Processed 23200/84484\n",
      "Processed 23300/84484\n",
      "Processed 23400/84484\n",
      "Processed 23500/84484\n",
      "Processed 23600/84484\n",
      "Processed 23700/84484\n",
      "Processed 23800/84484\n",
      "Processed 23900/84484\n",
      "Processed 24000/84484\n",
      "Processed 24100/84484\n",
      "Processed 24200/84484\n",
      "Processed 24300/84484\n",
      "Processed 24400/84484\n",
      "Processed 24500/84484\n",
      "Processed 24600/84484\n",
      "Processed 24700/84484\n",
      "Processed 24800/84484\n",
      "Processed 24900/84484\n",
      "Processed 25000/84484\n",
      "Processed 25100/84484\n",
      "Processed 25200/84484\n",
      "Processed 25300/84484\n",
      "Processed 25400/84484\n",
      "Processed 25500/84484\n",
      "Processed 25600/84484\n",
      "Processed 25700/84484\n",
      "Processed 25800/84484\n",
      "Processed 25900/84484\n",
      "Processed 26000/84484\n",
      "Processed 26100/84484\n",
      "Processed 26200/84484\n",
      "Processed 26300/84484\n",
      "Processed 26400/84484\n",
      "Processed 26500/84484\n",
      "Processed 26600/84484\n",
      "Processed 26700/84484\n",
      "Processed 26800/84484\n",
      "Processed 26900/84484\n",
      "Processed 27000/84484\n",
      "Processed 27100/84484\n",
      "Processed 27200/84484\n",
      "Processed 27300/84484\n",
      "Processed 27400/84484\n",
      "Processed 27500/84484\n",
      "Processed 27600/84484\n",
      "Processed 27700/84484\n",
      "Processed 27800/84484\n",
      "Processed 27900/84484\n",
      "Processed 28000/84484\n",
      "Processed 28100/84484\n",
      "Processed 28200/84484\n",
      "Processed 28300/84484\n",
      "Processed 28400/84484\n",
      "Processed 28500/84484\n",
      "Processed 28600/84484\n",
      "Processed 28700/84484\n",
      "Processed 28800/84484\n",
      "Processed 28900/84484\n",
      "Processed 29000/84484\n",
      "Processed 29100/84484\n",
      "Processed 29200/84484\n",
      "Processed 29300/84484\n",
      "Processed 29400/84484\n",
      "Processed 29500/84484\n",
      "Processed 29600/84484\n",
      "Processed 29700/84484\n",
      "Processed 29800/84484\n",
      "Processed 29900/84484\n",
      "Processed 30000/84484\n",
      "Processed 30100/84484\n",
      "Processed 30200/84484\n",
      "Processed 30300/84484\n",
      "Processed 30400/84484\n",
      "Processed 30500/84484\n",
      "Processed 30600/84484\n",
      "Processed 30700/84484\n",
      "Processed 30800/84484\n",
      "Processed 30900/84484\n",
      "Processed 31000/84484\n",
      "Processed 31100/84484\n",
      "Processed 31200/84484\n",
      "Processed 31300/84484\n",
      "Processed 31400/84484\n",
      "Processed 31500/84484\n",
      "Processed 31600/84484\n",
      "Processed 31700/84484\n",
      "Processed 31800/84484\n",
      "Processed 31900/84484\n",
      "Processed 32000/84484\n",
      "Processed 32100/84484\n",
      "Processed 32200/84484\n",
      "Processed 32300/84484\n",
      "Processed 32400/84484\n",
      "Processed 32500/84484\n",
      "Processed 32600/84484\n",
      "Processed 32700/84484\n",
      "Processed 32800/84484\n",
      "Processed 32900/84484\n",
      "Processed 33000/84484\n",
      "Processed 33100/84484\n",
      "Processed 33200/84484\n",
      "Processed 33300/84484\n",
      "Processed 33400/84484\n",
      "Processed 33500/84484\n",
      "Processed 33600/84484\n",
      "Processed 33700/84484\n",
      "Processed 33800/84484\n",
      "Processed 33900/84484\n",
      "Processed 34000/84484\n",
      "Processed 34100/84484\n",
      "Processed 34200/84484\n",
      "Processed 34300/84484\n",
      "Processed 34400/84484\n",
      "Processed 34500/84484\n",
      "Processed 34600/84484\n",
      "Processed 34700/84484\n",
      "Processed 34800/84484\n",
      "Processed 34900/84484\n",
      "Processed 35000/84484\n",
      "Processed 35100/84484\n",
      "Processed 35200/84484\n",
      "Processed 35300/84484\n",
      "Processed 35400/84484\n",
      "Processed 35500/84484\n",
      "Processed 35600/84484\n",
      "Processed 35700/84484\n",
      "Processed 35800/84484\n",
      "Processed 35900/84484\n",
      "Processed 36000/84484\n",
      "Processed 36100/84484\n",
      "Processed 36200/84484\n",
      "Processed 36300/84484\n",
      "Processed 36400/84484\n",
      "Processed 36500/84484\n",
      "Processed 36600/84484\n",
      "Processed 36700/84484\n",
      "Processed 36800/84484\n",
      "Processed 36900/84484\n",
      "Processed 37000/84484\n",
      "Processed 37100/84484\n",
      "Processed 37200/84484\n",
      "Processed 37300/84484\n",
      "Processed 37400/84484\n",
      "Processed 37500/84484\n",
      "Processed 37600/84484\n",
      "Processed 37700/84484\n",
      "Processed 37800/84484\n",
      "Processed 37900/84484\n",
      "Processed 38000/84484\n",
      "Processed 38100/84484\n",
      "Processed 38200/84484\n",
      "Processed 38300/84484\n",
      "Processed 38400/84484\n",
      "Processed 38500/84484\n",
      "Processed 38600/84484\n",
      "Processed 38700/84484\n",
      "Processed 38800/84484\n",
      "Processed 38900/84484\n",
      "Processed 39000/84484\n",
      "Processed 39100/84484\n",
      "Processed 39200/84484\n",
      "Processed 39300/84484\n",
      "Processed 39400/84484\n",
      "Processed 39500/84484\n",
      "Processed 39600/84484\n",
      "Processed 39700/84484\n",
      "Processed 39800/84484\n",
      "Processed 39900/84484\n",
      "Processed 40000/84484\n",
      "Processed 40100/84484\n",
      "Processed 40200/84484\n",
      "Processed 40300/84484\n",
      "Processed 40400/84484\n",
      "Processed 40500/84484\n",
      "Processed 40600/84484\n",
      "Processed 40700/84484\n",
      "Processed 40800/84484\n",
      "Processed 40900/84484\n",
      "Processed 41000/84484\n",
      "Processed 41100/84484\n",
      "Processed 41200/84484\n",
      "Processed 41300/84484\n",
      "Processed 41400/84484\n",
      "Processed 41500/84484\n",
      "Processed 41600/84484\n",
      "Processed 41700/84484\n",
      "Processed 41800/84484\n",
      "Processed 41900/84484\n",
      "Processed 42000/84484\n",
      "Processed 42100/84484\n",
      "Processed 42200/84484\n",
      "Processed 42300/84484\n",
      "Processed 42400/84484\n",
      "Processed 42500/84484\n",
      "Processed 42600/84484\n",
      "Processed 42700/84484\n",
      "Processed 42800/84484\n",
      "Processed 42900/84484\n",
      "Processed 43000/84484\n",
      "Processed 43100/84484\n",
      "Processed 43200/84484\n",
      "Processed 43300/84484\n",
      "Processed 43400/84484\n",
      "Processed 43500/84484\n",
      "Processed 43600/84484\n",
      "Processed 43700/84484\n",
      "Processed 43800/84484\n",
      "Processed 43900/84484\n",
      "Processed 44000/84484\n",
      "Processed 44100/84484\n",
      "Processed 44200/84484\n",
      "Processed 44300/84484\n",
      "Processed 44400/84484\n",
      "Processed 44500/84484\n",
      "Processed 44600/84484\n",
      "Processed 44700/84484\n",
      "Processed 44800/84484\n",
      "Processed 44900/84484\n",
      "Processed 45000/84484\n",
      "Processed 45100/84484\n",
      "Processed 45200/84484\n",
      "Processed 45300/84484\n",
      "Processed 45400/84484\n",
      "Processed 45500/84484\n",
      "Processed 45600/84484\n",
      "Processed 45700/84484\n",
      "Processed 45800/84484\n",
      "Processed 45900/84484\n",
      "Processed 46000/84484\n",
      "Processed 46100/84484\n",
      "Processed 46200/84484\n",
      "Processed 46300/84484\n",
      "Processed 46400/84484\n",
      "Processed 46500/84484\n",
      "Processed 46600/84484\n",
      "Processed 46700/84484\n",
      "Processed 46800/84484\n",
      "Processed 46900/84484\n",
      "Processed 47000/84484\n",
      "Processed 47100/84484\n",
      "Processed 47200/84484\n",
      "Processed 47300/84484\n",
      "Processed 47400/84484\n",
      "Processed 47500/84484\n",
      "Processed 47600/84484\n",
      "Processed 47700/84484\n",
      "Processed 47800/84484\n",
      "Processed 47900/84484\n",
      "Processed 48000/84484\n",
      "Processed 48100/84484\n",
      "Processed 48200/84484\n",
      "Processed 48300/84484\n",
      "Processed 48400/84484\n",
      "Processed 48500/84484\n",
      "Processed 48600/84484\n",
      "Processed 48700/84484\n",
      "Processed 48800/84484\n",
      "Processed 48900/84484\n",
      "Processed 49000/84484\n",
      "Processed 49100/84484\n",
      "Processed 49200/84484\n",
      "Processed 49300/84484\n",
      "Processed 49400/84484\n",
      "Processed 49500/84484\n",
      "Processed 49600/84484\n",
      "Processed 49700/84484\n",
      "Processed 49800/84484\n",
      "Processed 49900/84484\n",
      "Processed 50000/84484\n",
      "Processed 50100/84484\n",
      "Processed 50200/84484\n",
      "Processed 50300/84484\n",
      "Processed 50400/84484\n",
      "Processed 50500/84484\n",
      "Processed 50600/84484\n",
      "Processed 50700/84484\n",
      "Processed 50800/84484\n",
      "Processed 50900/84484\n",
      "Processed 51000/84484\n",
      "Processed 51100/84484\n",
      "Processed 51200/84484\n",
      "Processed 51300/84484\n",
      "Processed 51400/84484\n",
      "Processed 51500/84484\n",
      "Processed 51600/84484\n",
      "Processed 51700/84484\n",
      "Processed 51800/84484\n",
      "Processed 51900/84484\n",
      "Processed 52000/84484\n",
      "Processed 52100/84484\n",
      "Processed 52200/84484\n",
      "Processed 52300/84484\n",
      "Processed 52400/84484\n",
      "Processed 52500/84484\n",
      "Processed 52600/84484\n",
      "Processed 52700/84484\n",
      "Processed 52800/84484\n",
      "Processed 52900/84484\n",
      "Processed 53000/84484\n",
      "Processed 53100/84484\n",
      "Processed 53200/84484\n",
      "Processed 53300/84484\n",
      "Processed 53400/84484\n",
      "Processed 53500/84484\n",
      "Processed 53600/84484\n",
      "Processed 53700/84484\n",
      "Processed 53800/84484\n",
      "Processed 53900/84484\n",
      "Processed 54000/84484\n",
      "Processed 54100/84484\n",
      "Processed 54200/84484\n",
      "Processed 54300/84484\n",
      "Processed 54400/84484\n",
      "Processed 54500/84484\n",
      "Processed 54600/84484\n",
      "Processed 54700/84484\n",
      "Processed 54800/84484\n",
      "Processed 54900/84484\n",
      "Processed 55000/84484\n",
      "Processed 55100/84484\n",
      "Processed 55200/84484\n",
      "Processed 55300/84484\n",
      "Processed 55400/84484\n",
      "Processed 55500/84484\n",
      "Processed 55600/84484\n",
      "Processed 55700/84484\n",
      "Processed 55800/84484\n",
      "Processed 55900/84484\n",
      "Processed 56000/84484\n",
      "Processed 56100/84484\n",
      "Processed 56200/84484\n",
      "Processed 56300/84484\n",
      "Processed 56400/84484\n",
      "Processed 56500/84484\n",
      "Processed 56600/84484\n",
      "Processed 56700/84484\n",
      "Processed 56800/84484\n",
      "Processed 56900/84484\n",
      "Processed 57000/84484\n",
      "Processed 57100/84484\n",
      "Processed 57200/84484\n",
      "Processed 57300/84484\n",
      "Processed 57400/84484\n",
      "Processed 57500/84484\n",
      "Processed 57600/84484\n",
      "Processed 57700/84484\n",
      "Processed 57800/84484\n",
      "Processed 57900/84484\n",
      "Processed 58000/84484\n",
      "Processed 58100/84484\n",
      "Processed 58200/84484\n",
      "Processed 58300/84484\n",
      "Processed 58400/84484\n",
      "Processed 58500/84484\n",
      "Processed 58600/84484\n",
      "Processed 58700/84484\n",
      "Processed 58800/84484\n",
      "Processed 58900/84484\n",
      "Processed 59000/84484\n",
      "Processed 59100/84484\n",
      "Processed 59200/84484\n",
      "Processed 59300/84484\n",
      "Processed 59400/84484\n",
      "Processed 59500/84484\n",
      "Processed 59600/84484\n",
      "Processed 59700/84484\n",
      "Processed 59800/84484\n",
      "Processed 59900/84484\n",
      "Processed 60000/84484\n",
      "Processed 60100/84484\n",
      "Processed 60200/84484\n",
      "Processed 60300/84484\n",
      "Processed 60400/84484\n",
      "Processed 60500/84484\n",
      "Processed 60600/84484\n",
      "Processed 60700/84484\n",
      "Processed 60800/84484\n",
      "Processed 60900/84484\n",
      "Processed 61000/84484\n",
      "Processed 61100/84484\n",
      "Processed 61200/84484\n",
      "Processed 61300/84484\n",
      "Processed 61400/84484\n",
      "Processed 61500/84484\n",
      "Processed 61600/84484\n",
      "Processed 61700/84484\n",
      "Processed 61800/84484\n",
      "Processed 61900/84484\n",
      "Processed 62000/84484\n",
      "Processed 62100/84484\n",
      "Processed 62200/84484\n",
      "Processed 62300/84484\n",
      "Processed 62400/84484\n",
      "Processed 62500/84484\n",
      "Processed 62600/84484\n",
      "Processed 62700/84484\n",
      "Processed 62800/84484\n",
      "Processed 62900/84484\n",
      "Processed 63000/84484\n",
      "Processed 63100/84484\n",
      "Processed 63200/84484\n",
      "Processed 63300/84484\n",
      "Processed 63400/84484\n",
      "Processed 63500/84484\n",
      "Processed 63600/84484\n",
      "Processed 63700/84484\n",
      "Processed 63800/84484\n",
      "Processed 63900/84484\n",
      "Processed 64000/84484\n",
      "Processed 64100/84484\n",
      "Processed 64200/84484\n",
      "Processed 64300/84484\n",
      "Processed 64400/84484\n",
      "Processed 64500/84484\n",
      "Processed 64600/84484\n",
      "Processed 64700/84484\n",
      "Processed 64800/84484\n",
      "Processed 64900/84484\n",
      "Processed 65000/84484\n",
      "Processed 65100/84484\n",
      "Processed 65200/84484\n",
      "Processed 65300/84484\n",
      "Processed 65400/84484\n",
      "Processed 65500/84484\n",
      "Processed 65600/84484\n",
      "Processed 65700/84484\n",
      "Processed 65800/84484\n",
      "Processed 65900/84484\n",
      "Processed 66000/84484\n",
      "Processed 66100/84484\n",
      "Processed 66200/84484\n",
      "Processed 66300/84484\n",
      "Processed 66400/84484\n",
      "Processed 66500/84484\n",
      "Processed 66600/84484\n",
      "Processed 66700/84484\n",
      "Processed 66800/84484\n",
      "Processed 66900/84484\n",
      "Processed 67000/84484\n",
      "Processed 67100/84484\n",
      "Processed 67200/84484\n",
      "Processed 67300/84484\n",
      "Processed 67400/84484\n",
      "Processed 67500/84484\n",
      "Processed 67600/84484\n",
      "Processed 67700/84484\n",
      "Processed 67800/84484\n",
      "Processed 67900/84484\n",
      "Processed 68000/84484\n",
      "Processed 68100/84484\n",
      "Processed 68200/84484\n",
      "Processed 68300/84484\n",
      "Processed 68400/84484\n",
      "Processed 68500/84484\n",
      "Processed 68600/84484\n",
      "Processed 68700/84484\n",
      "Processed 68800/84484\n",
      "Processed 68900/84484\n",
      "Processed 69000/84484\n",
      "Processed 69100/84484\n",
      "Processed 69200/84484\n",
      "Processed 69300/84484\n",
      "Processed 69400/84484\n",
      "Processed 69500/84484\n",
      "Processed 69600/84484\n",
      "Processed 69700/84484\n",
      "Processed 69800/84484\n",
      "Processed 69900/84484\n",
      "Processed 70000/84484\n",
      "Processed 70100/84484\n",
      "Processed 70200/84484\n",
      "Processed 70300/84484\n",
      "Processed 70400/84484\n",
      "Processed 70500/84484\n",
      "Processed 70600/84484\n",
      "Processed 70700/84484\n",
      "Processed 70800/84484\n",
      "Processed 70900/84484\n",
      "Processed 71000/84484\n",
      "Processed 71100/84484\n",
      "Processed 71200/84484\n",
      "Processed 71300/84484\n",
      "Processed 71400/84484\n",
      "Processed 71500/84484\n",
      "Processed 71600/84484\n",
      "Processed 71700/84484\n",
      "Processed 71800/84484\n",
      "Processed 71900/84484\n",
      "Processed 72000/84484\n",
      "Processed 72100/84484\n",
      "Processed 72200/84484\n",
      "Processed 72300/84484\n",
      "Processed 72400/84484\n",
      "Processed 72500/84484\n",
      "Processed 72600/84484\n",
      "Processed 72700/84484\n",
      "Processed 72800/84484\n",
      "Processed 72900/84484\n",
      "Processed 73000/84484\n",
      "Processed 73100/84484\n",
      "Processed 73200/84484\n",
      "Processed 73300/84484\n",
      "Processed 73400/84484\n",
      "Processed 73500/84484\n",
      "Processed 73600/84484\n",
      "Processed 73700/84484\n",
      "Processed 73800/84484\n",
      "Processed 73900/84484\n",
      "Processed 74000/84484\n",
      "Processed 74100/84484\n",
      "Processed 74200/84484\n",
      "Processed 74300/84484\n",
      "Processed 74400/84484\n",
      "Processed 74500/84484\n",
      "Processed 74600/84484\n",
      "Processed 74700/84484\n",
      "Processed 74800/84484\n",
      "Processed 74900/84484\n",
      "Processed 75000/84484\n",
      "Processed 75100/84484\n",
      "Processed 75200/84484\n",
      "Processed 75300/84484\n",
      "Processed 75400/84484\n",
      "Processed 75500/84484\n",
      "Processed 75600/84484\n",
      "Processed 75700/84484\n",
      "Processed 75800/84484\n",
      "Processed 75900/84484\n",
      "Processed 76000/84484\n",
      "Processed 76100/84484\n",
      "Processed 76200/84484\n",
      "Processed 76300/84484\n",
      "Processed 76400/84484\n",
      "Processed 76500/84484\n",
      "Processed 76600/84484\n",
      "Processed 76700/84484\n",
      "Processed 76800/84484\n",
      "Processed 76900/84484\n",
      "Processed 77000/84484\n",
      "Processed 77100/84484\n",
      "Processed 77200/84484\n",
      "Processed 77300/84484\n",
      "Processed 77400/84484\n",
      "Processed 77500/84484\n",
      "Processed 77600/84484\n",
      "Processed 77700/84484\n",
      "Processed 77800/84484\n",
      "Processed 77900/84484\n",
      "Processed 78000/84484\n",
      "Processed 78100/84484\n",
      "Processed 78200/84484\n",
      "Processed 78300/84484\n",
      "Processed 78400/84484\n",
      "Processed 78500/84484\n",
      "Processed 78600/84484\n",
      "Processed 78700/84484\n",
      "Processed 78800/84484\n",
      "Processed 78900/84484\n",
      "Processed 79000/84484\n",
      "Processed 79100/84484\n",
      "Processed 79200/84484\n",
      "Processed 79300/84484\n",
      "Processed 79400/84484\n",
      "Processed 79500/84484\n",
      "Processed 79600/84484\n",
      "Processed 79700/84484\n",
      "Processed 79800/84484\n",
      "Processed 79900/84484\n",
      "Processed 80000/84484\n",
      "Processed 80100/84484\n",
      "Processed 80200/84484\n",
      "Processed 80300/84484\n",
      "Processed 80400/84484\n",
      "Processed 80500/84484\n",
      "Processed 80600/84484\n",
      "Processed 80700/84484\n",
      "Processed 80800/84484\n",
      "Processed 80900/84484\n",
      "Processed 81000/84484\n",
      "Processed 81100/84484\n",
      "Processed 81200/84484\n",
      "Processed 81300/84484\n",
      "Processed 81400/84484\n",
      "Processed 81500/84484\n",
      "Processed 81600/84484\n",
      "Processed 81700/84484\n",
      "Processed 81800/84484\n",
      "Processed 81900/84484\n",
      "Processed 82000/84484\n",
      "Processed 82100/84484\n",
      "Processed 82200/84484\n",
      "Processed 82300/84484\n",
      "Processed 82400/84484\n",
      "Processed 82500/84484\n",
      "Processed 82600/84484\n",
      "Processed 82700/84484\n",
      "Processed 82800/84484\n",
      "Processed 82900/84484\n",
      "Processed 83000/84484\n",
      "Processed 83100/84484\n",
      "Processed 83200/84484\n",
      "Processed 83300/84484\n",
      "Processed 83400/84484\n",
      "Processed 83500/84484\n",
      "Processed 83600/84484\n",
      "Processed 83700/84484\n",
      "Processed 83800/84484\n",
      "Processed 83900/84484\n",
      "Processed 84000/84484\n",
      "Processed 84100/84484\n",
      "Processed 84200/84484\n",
      "Processed 84300/84484\n",
      "Processed 84400/84484\n",
      "✅ All images saved to: C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_Resize\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- CONFIG --------\n",
    "INPUT_ROOT  = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\research\\dataset\\OCT2017_CLEAN\"\n",
    "OUTPUT_ROOT = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_Resize\"\n",
    "TARGET_SIZE = (224, 224)\n",
    "EXTS = {\".png\", \".jpg\", \".jpeg\"}   # accepted formats\n",
    "# ------------------------\n",
    "\n",
    "def resize_with_padding(img, target_size=(224, 224)):\n",
    "    h, w = img.shape[:2]\n",
    "    tw, th = target_size\n",
    "    scale = min(tw / w, th / h)\n",
    "    nw, nh = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "    dw, dh = tw - nw, th - nh\n",
    "    top, bottom = dh // 2, dh - dh // 2\n",
    "    left, right = dw // 2, dw - dw // 2\n",
    "    return cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
    "                              cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "def process_dataset(in_root, out_root):\n",
    "    in_root, out_root = Path(in_root), Path(out_root)\n",
    "    all_imgs = [p for p in in_root.rglob(\"*\") if p.suffix.lower() in EXTS]\n",
    "\n",
    "    if not in_root.exists():\n",
    "        print(\"❌ INPUT_ROOT not found:\", in_root)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(all_imgs)} images under {in_root}\")\n",
    "    for i, src in enumerate(all_imgs, 1):\n",
    "        rel = src.relative_to(in_root)\n",
    "        dst = out_root / rel\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        img = cv2.imread(str(src))\n",
    "        if img is None:\n",
    "            print(\"⚠️ Skipping unreadable file:\", src)\n",
    "            continue\n",
    "        out = resize_with_padding(img, TARGET_SIZE)\n",
    "        cv2.imwrite(str(dst), out)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(all_imgs)}\")\n",
    "\n",
    "    print(\"✅ All images saved to:\", out_root)\n",
    "\n",
    "process_dataset(INPUT_ROOT, OUTPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aca9c",
   "metadata": {},
   "source": [
    "# Preprocessing Step 2: White-Border Removal\n",
    "Some pipelines introduce white borders during previous processing or export.\n",
    "These high-intensity edges can bias the model and distort normalization.\n",
    "\n",
    "We detect **white regions that touch the image border** via connected components\n",
    "and set them to **black**. Outputs are saved to `OCT2017_border`. A summary reports\n",
    "how many files were modified per split/class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8cb0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84484 images. Writing to: C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_border\n",
      "[██████████████████████████████] 84484/84484\n",
      "\n",
      "✅ Done.\n",
      "Processed images : 84484\n",
      "Skipped (exists) : 0\n",
      "Errors           : 0\n",
      "Pixels changed   : 160997002\n",
      "Elapsed          : 2064.0s\n",
      "\n",
      "Summary per split/class (files seen, files modified):\n",
      "  test/CNV                 242  modified:   181\n",
      "  test/DME                 242  modified:   147\n",
      "  test/DRUSEN              242  modified:   204\n",
      "  test/NORMAL              242  modified:   221\n",
      "  train/CNV              37205  modified: 21571\n",
      "  train/DME              11348  modified:  7033\n",
      "  train/DRUSEN            8616  modified:  5666\n",
      "  train/NORMAL           26315  modified: 16548\n",
      "  val/CNV                    8  modified:     5\n",
      "  val/DME                    8  modified:     5\n",
      "  val/DRUSEN                 8  modified:     6\n",
      "  val/NORMAL                 8  modified:     7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_ROOT  = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_Resize\"\n",
    "OUTPUT_ROOT = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_border\"\n",
    "EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "OVERWRITE = False\n",
    "WHITE_THRESH = 250  # pixels >= this are considered 'white' for border detection\n",
    "# ---------------------------\n",
    "\n",
    "def fill_white_border_black(img, white_thresh=250):\n",
    "    \"\"\"\n",
    "    Replace white regions that touch the image border with black.\n",
    "    Preserves grayscale or color (BGR) shape.\n",
    "    Returns: (modified_img, changed_pixels_count)\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Build 'white' mask (grayscale or color)\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        white_mask = np.all(img >= white_thresh, axis=2)\n",
    "    else:\n",
    "        white_mask = (img >= white_thresh)\n",
    "\n",
    "    if not np.any(white_mask):\n",
    "        return img, 0\n",
    "\n",
    "    white_u8 = (white_mask.astype(np.uint8) * 255)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(white_u8, connectivity=8)\n",
    "\n",
    "    # find components that touch any border\n",
    "    border_labels = []\n",
    "    for lbl in range(1, num_labels):  # skip background\n",
    "        left = stats[lbl, cv2.CC_STAT_LEFT]\n",
    "        top = stats[lbl, cv2.CC_STAT_TOP]\n",
    "        width = stats[lbl, cv2.CC_STAT_WIDTH]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if left == 0 or top == 0 or (left + width) == w or (top + height) == h:\n",
    "            border_labels.append(lbl)\n",
    "\n",
    "    if len(border_labels) == 0:\n",
    "        # fallback: keep only whites that lie exactly on the outermost edges\n",
    "        full_border_mask = np.zeros_like(white_mask, dtype=bool)\n",
    "        full_border_mask[0, :]  = white_mask[0, :]\n",
    "        full_border_mask[-1, :] = white_mask[-1, :]\n",
    "        full_border_mask[:, 0]  = full_border_mask[:, 0]  | white_mask[:, 0]\n",
    "        full_border_mask[:, -1] = full_border_mask[:, -1] | white_mask[:, -1]\n",
    "    else:\n",
    "        # include all pixels belonging to border-touching white components\n",
    "        full_border_mask = np.isin(labels, border_labels)\n",
    "\n",
    "    target_mask = white_mask & full_border_mask\n",
    "    changed = int(np.count_nonzero(target_mask))\n",
    "    if changed == 0:\n",
    "        return img, 0\n",
    "\n",
    "    out = img.copy()\n",
    "    if out.ndim == 3:\n",
    "        out[target_mask, :] = 0\n",
    "    else:\n",
    "        out[target_mask] = 0\n",
    "    return out, changed\n",
    "\n",
    "def iter_images(root, exts):\n",
    "    root = Path(root)\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            yield p\n",
    "\n",
    "def main():\n",
    "    in_root  = Path(INPUT_ROOT)\n",
    "    out_root = Path(OUTPUT_ROOT)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not in_root.exists():\n",
    "        print(\"❌ INPUT_ROOT does not exist:\", in_root)\n",
    "        print(\"Tip: If this is a OneDrive folder, right-click it in Explorer and choose 'Always keep on this device'.\")\n",
    "        return\n",
    "\n",
    "    files = list(iter_images(in_root, EXTS))\n",
    "    total = len(files)\n",
    "    if total == 0:\n",
    "        print(\"❌ No images found under:\", in_root)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {total} images. Writing to: {out_root}\")\n",
    "    t0 = time()\n",
    "    processed = skipped = errors = 0\n",
    "    changed_pixels_total = 0\n",
    "\n",
    "    # Optional per (split,class) counts just for reporting\n",
    "    buckets = defaultdict(lambda: {\"count\":0, \"changed\":0})\n",
    "\n",
    "    def progress(i, n, bar_len=30):\n",
    "        frac = (i + 1) / n\n",
    "        bar = \"█\" * int(bar_len * frac) + \"·\" * (bar_len - int(bar_len * frac))\n",
    "        print(f\"\\r[{bar}] {i+1}/{n}\", end=\"\", flush=True)\n",
    "\n",
    "    for i, src in enumerate(files):\n",
    "        rel = src.relative_to(in_root)\n",
    "        dst = out_root / rel\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # bucket label for quick sanity: e.g., train/CNV or val/DME\n",
    "        parts = rel.parts\n",
    "        if len(parts) >= 2 and parts[0].lower() in {\"train\",\"val\",\"test\"}:\n",
    "            bucket = f\"{parts[0]}/{parts[1]}\"\n",
    "        elif len(parts) >= 1:\n",
    "            bucket = parts[0]\n",
    "        else:\n",
    "            bucket = \"root\"\n",
    "\n",
    "        if dst.exists() and not OVERWRITE:\n",
    "            skipped += 1\n",
    "            buckets[bucket][\"count\"] += 1\n",
    "            progress(i, total)\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(src), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            errors += 1\n",
    "            progress(i, total)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            out, changed = fill_white_border_black(img, white_thresh=WHITE_THRESH)\n",
    "            cv2.imwrite(str(dst), out)\n",
    "            processed += 1\n",
    "            changed_pixels_total += changed\n",
    "            buckets[bucket][\"count\"] += 1\n",
    "            buckets[bucket][\"changed\"] += (1 if changed > 0 else 0)\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "        finally:\n",
    "            progress(i, total)\n",
    "\n",
    "    dt = time() - t0\n",
    "    print(\"\\n\\n✅ Done.\")\n",
    "    print(f\"Processed images : {processed}\")\n",
    "    print(f\"Skipped (exists) : {skipped}\")\n",
    "    print(f\"Errors           : {errors}\")\n",
    "    print(f\"Pixels changed   : {changed_pixels_total}\")\n",
    "    print(f\"Elapsed          : {dt:.1f}s\")\n",
    "\n",
    "    # Small summary per bucket\n",
    "    print(\"\\nSummary per split/class (files seen, files modified):\")\n",
    "    for k in sorted(buckets.keys()):\n",
    "        print(f\"  {k:<20}  {buckets[k]['count']:>6}  modified:{buckets[k]['changed']:>6}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7821a72",
   "metadata": {},
   "source": [
    "# Split Step 1: Stratified Train/Val (from Border-Cleaned Set)\n",
    "We construct a new validation set (default **15%**) *per class* from the border-cleaned dataset.\n",
    "Optionally, the old `val/` is merged back into the pool to increase sample variety.\n",
    "\n",
    "- Input: `OCT2017_border`\n",
    "- Output: `OCT2017_STRATIFIED_BORDER`\n",
    "- Test is **copied unchanged**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9da369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Stratified split complete (using BORDER dataset).\n",
      "Output root: C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_STRATIFIED_BORDER\n",
      " New train/: 70989 images\n",
      " New val/  : 12527 images\n",
      " test/     : 968 images (copied from input/test)\n",
      "\n",
      "Per-class breakdown (pool -> train / val):\n",
      "  CNV        pool= 37213  train= 31631  val=  5582  (val 15.0%)\n",
      "  DME        pool= 11356  train=  9653  val=  1703  (val 15.0%)\n",
      "  DRUSEN     pool=  8624  train=  7330  val=  1294  (val 15.0%)\n",
      "  NORMAL     pool= 26323  train= 22375  val=  3948  (val 15.0%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "# Use your border-cleaned dataset as input\n",
    "INPUT_ROOT   = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_border\"\n",
    "\n",
    "# Output root where new train/ val/ test/ will be created\n",
    "OUTPUT_ROOT  = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_STRATIFIED_BORDER\"\n",
    "\n",
    "VAL_RATIO    = 0.15     # 15% of (train + old val, if included) -> validation\n",
    "INCLUDE_OLD_VAL_IN_POOL = True  # True = merge INPUT_ROOT/val back into pool before splitting\n",
    "COPY_TEST    = True     # copy INPUT_ROOT/test to OUTPUT_ROOT/test as-is\n",
    "\n",
    "RANDOM_SEED  = 42\n",
    "CLEAN_OUTPUT = False    # CAUTION: if True, deletes OUTPUT_ROOT before writing\n",
    "EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "# ============================================\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "in_root   = Path(INPUT_ROOT)\n",
    "out_root  = Path(OUTPUT_ROOT)\n",
    "train_in  = in_root / \"train\"\n",
    "val_in    = in_root / \"val\"\n",
    "test_in   = in_root / \"test\"\n",
    "\n",
    "train_out = out_root / \"train\"\n",
    "val_out   = out_root / \"val\"\n",
    "test_out  = out_root / \"test\"\n",
    "\n",
    "def list_images(folder: Path):\n",
    "    return sorted([p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in EXTS])\n",
    "\n",
    "def copy_files(files, dst_dir: Path):\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in files:\n",
    "        shutil.copy2(f, dst_dir / f.name)\n",
    "\n",
    "def copy_tree(src: Path, dst: Path):\n",
    "    if not src.exists():\n",
    "        print(f\"⚠️ Source not found, skipping copy: {src}\")\n",
    "        return\n",
    "    for cls_dir in [d for d in src.iterdir() if d.is_dir()]:\n",
    "        cls_out = dst / cls_dir.name\n",
    "        cls_out.mkdir(parents=True, exist_ok=True)\n",
    "        for f in list_images(cls_dir):\n",
    "            shutil.copy2(f, cls_out / f.name)\n",
    "\n",
    "def collect_by_class(root: Path):\n",
    "    \"\"\"Return dict: class_name -> [Path, ...] from immediate subfolders.\"\"\"\n",
    "    if not root.exists():\n",
    "        return {}\n",
    "    classes = [d for d in root.iterdir() if d.is_dir()]\n",
    "    by_class = {}\n",
    "    for c in classes:\n",
    "        imgs = list_images(c)\n",
    "        if imgs:\n",
    "            by_class[c.name] = imgs\n",
    "    return by_class\n",
    "\n",
    "# ---------- Safety & setup ----------\n",
    "if not in_root.exists():\n",
    "    raise FileNotFoundError(f\"INPUT_ROOT not found: {in_root}\\n\"\n",
    "                            f\"Tip: If OneDrive, right-click folder in Explorer -> 'Always keep on this device'.\")\n",
    "\n",
    "if out_root.exists() and CLEAN_OUTPUT:\n",
    "    print(f\"⚠️ Removing existing output: {out_root}\")\n",
    "    shutil.rmtree(out_root)\n",
    "\n",
    "train_out.mkdir(parents=True, exist_ok=True)\n",
    "val_out.mkdir(parents=True, exist_ok=True)\n",
    "if COPY_TEST:\n",
    "    test_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Gather pool for stratified split ----------\n",
    "pool_by_class = defaultdict(list)\n",
    "\n",
    "# add TRAIN\n",
    "train_by_class = collect_by_class(train_in)\n",
    "if not train_by_class:\n",
    "    raise RuntimeError(f\"No class subfolders or images found under {train_in}\")\n",
    "\n",
    "for cls, files in train_by_class.items():\n",
    "    pool_by_class[cls].extend(files)\n",
    "\n",
    "# optionally add old VAL into pool\n",
    "if INCLUDE_OLD_VAL_IN_POOL and val_in.exists():\n",
    "    val_by_class = collect_by_class(val_in)\n",
    "    for cls, files in val_by_class.items():\n",
    "        pool_by_class[cls].extend(files)\n",
    "\n",
    "# ---------- Stratified split ----------\n",
    "summary = defaultdict(lambda: {\"pool\":0, \"train\":0, \"val\":0})\n",
    "\n",
    "for cls, files in pool_by_class.items():\n",
    "    if len(files) == 0:\n",
    "        print(f\"⚠️ No images for class '{cls}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(files)\n",
    "    n_total = len(files)\n",
    "    n_val = max(1, int(round(n_total * VAL_RATIO)))\n",
    "\n",
    "    val_files   = files[:n_val]\n",
    "    train_files = files[n_val:]\n",
    "\n",
    "    # write out\n",
    "    cls_train_out = train_out / cls\n",
    "    cls_val_out   = val_out / cls\n",
    "    cls_train_out.mkdir(parents=True, exist_ok=True)\n",
    "    cls_val_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for src in train_files:\n",
    "        shutil.copy2(src, cls_train_out / src.name)\n",
    "    for src in val_files:\n",
    "        shutil.copy2(src, cls_val_out / src.name)\n",
    "\n",
    "    # summary\n",
    "    summary[cls][\"pool\"]  = n_total\n",
    "    summary[cls][\"train\"] = len(train_files)\n",
    "    summary[cls][\"val\"]   = len(val_files)\n",
    "\n",
    "# ---------- Copy test split unchanged ----------\n",
    "if COPY_TEST and test_in.exists():\n",
    "    for cls_dir in [d for d in test_in.iterdir() if d.is_dir()]:\n",
    "        copy_files(list_images(cls_dir), test_out / cls_dir.name)\n",
    "\n",
    "# ---------- Report ----------\n",
    "grand_pool  = sum(v[\"pool\"]  for v in summary.values())\n",
    "grand_train = sum(v[\"train\"] for v in summary.values())\n",
    "grand_val   = sum(v[\"val\"]   for v in summary.values())\n",
    "\n",
    "print(\"\\n✅ Stratified split complete (using BORDER dataset).\")\n",
    "print(f\"Output root: {out_root}\")\n",
    "print(f\" New train/: {grand_train} images\")\n",
    "print(f\" New val/  : {grand_val} images\")\n",
    "if COPY_TEST and test_in.exists():\n",
    "    test_count = len(list_images(test_out))\n",
    "    print(f\" test/     : {test_count} images (copied from input/test)\")\n",
    "print(\"\\nPer-class breakdown (pool -> train / val):\")\n",
    "for cls in sorted(summary.keys()):\n",
    "    s = summary[cls]\n",
    "    r = (s[\"val\"] / max(1, s[\"pool\"])) * 100.0\n",
    "    print(f\"  {cls:<10} pool={s['pool']:>6}  train={s['train']:>6}  val={s['val']:>6}  (val {r:4.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d5331",
   "metadata": {},
   "source": [
    "# Split Step 2: Global 70/15/15 (Train/Val/Test)\n",
    "For a clean experimental baseline, we pool **train/val/test** per class and\n",
    "re-split into **70% Train / 15% Val / 15% Test**. This produces balanced split\n",
    "sizes at the dataset level while preserving class stratification.\n",
    "\n",
    "- Input: `OCT2017_STRATIFIED_BORDER`\n",
    "- Output: `OCT2017_70_15_15`\n",
    "- Duplicate filenames are auto-resolved by appending a suffix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6db5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stratified 70/15/15 split per class...\n",
      "Copying 84465 files to:\n",
      "  C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_70_15_15\n",
      "This may take a while...\n",
      "  copied 200/84465 files…\n",
      "  copied 400/84465 files…\n",
      "  copied 600/84465 files…\n",
      "  copied 800/84465 files…\n",
      "  copied 1000/84465 files…\n",
      "  copied 1200/84465 files…\n",
      "  copied 1400/84465 files…\n",
      "  copied 1600/84465 files…\n",
      "  copied 1800/84465 files…\n",
      "  copied 2000/84465 files…\n",
      "  copied 2200/84465 files…\n",
      "  copied 2400/84465 files…\n",
      "  copied 2600/84465 files…\n",
      "  copied 2800/84465 files…\n",
      "  copied 3000/84465 files…\n",
      "  copied 3200/84465 files…\n",
      "  copied 3400/84465 files…\n",
      "  copied 3600/84465 files…\n",
      "  copied 3800/84465 files…\n",
      "  copied 4000/84465 files…\n",
      "  copied 4200/84465 files…\n",
      "  copied 4400/84465 files…\n",
      "  copied 4600/84465 files…\n",
      "  copied 4800/84465 files…\n",
      "  copied 5000/84465 files…\n",
      "  copied 5200/84465 files…\n",
      "  copied 5400/84465 files…\n",
      "  copied 5600/84465 files…\n",
      "  copied 5800/84465 files…\n",
      "  copied 6000/84465 files…\n",
      "  copied 6200/84465 files…\n",
      "  copied 6400/84465 files…\n",
      "  copied 6600/84465 files…\n",
      "  copied 6800/84465 files…\n",
      "  copied 7000/84465 files…\n",
      "  copied 7200/84465 files…\n",
      "  copied 7400/84465 files…\n",
      "  copied 7600/84465 files…\n",
      "  copied 7800/84465 files…\n",
      "  copied 8000/84465 files…\n",
      "  copied 8200/84465 files…\n",
      "  copied 8400/84465 files…\n",
      "  copied 8600/84465 files…\n",
      "  copied 8800/84465 files…\n",
      "  copied 9000/84465 files…\n",
      "  copied 9200/84465 files…\n",
      "  copied 9400/84465 files…\n",
      "  copied 9600/84465 files…\n",
      "  copied 9800/84465 files…\n",
      "  copied 10000/84465 files…\n",
      "  copied 10200/84465 files…\n",
      "  copied 10400/84465 files…\n",
      "  copied 10600/84465 files…\n",
      "  copied 10800/84465 files…\n",
      "  copied 11000/84465 files…\n",
      "  copied 11200/84465 files…\n",
      "  copied 11400/84465 files…\n",
      "  copied 11600/84465 files…\n",
      "  copied 11800/84465 files…\n",
      "  copied 12000/84465 files…\n",
      "  copied 12200/84465 files…\n",
      "  copied 12400/84465 files…\n",
      "  copied 12600/84465 files…\n",
      "  copied 12800/84465 files…\n",
      "  copied 13000/84465 files…\n",
      "  copied 13200/84465 files…\n",
      "  copied 13400/84465 files…\n",
      "  copied 13600/84465 files…\n",
      "  copied 13800/84465 files…\n",
      "  copied 14000/84465 files…\n",
      "  copied 14200/84465 files…\n",
      "  copied 14400/84465 files…\n",
      "  copied 14600/84465 files…\n",
      "  copied 14800/84465 files…\n",
      "  copied 15000/84465 files…\n",
      "  copied 15200/84465 files…\n",
      "  copied 15400/84465 files…\n",
      "  copied 15600/84465 files…\n",
      "  copied 15800/84465 files…\n",
      "  copied 16000/84465 files…\n",
      "  copied 16200/84465 files…\n",
      "  copied 16400/84465 files…\n",
      "  copied 16600/84465 files…\n",
      "  copied 16800/84465 files…\n",
      "  copied 17000/84465 files…\n",
      "  copied 17200/84465 files…\n",
      "  copied 17400/84465 files…\n",
      "  copied 17600/84465 files…\n",
      "  copied 17800/84465 files…\n",
      "  copied 18000/84465 files…\n",
      "  copied 18200/84465 files…\n",
      "  copied 18400/84465 files…\n",
      "  copied 18600/84465 files…\n",
      "  copied 18800/84465 files…\n",
      "  copied 19000/84465 files…\n",
      "  copied 19200/84465 files…\n",
      "  copied 19400/84465 files…\n",
      "  copied 19600/84465 files…\n",
      "  copied 19800/84465 files…\n",
      "  copied 20000/84465 files…\n",
      "  copied 20200/84465 files…\n",
      "  copied 20400/84465 files…\n",
      "  copied 20600/84465 files…\n",
      "  copied 20800/84465 files…\n",
      "  copied 21000/84465 files…\n",
      "  copied 21200/84465 files…\n",
      "  copied 21400/84465 files…\n",
      "  copied 21600/84465 files…\n",
      "  copied 21800/84465 files…\n",
      "  copied 22000/84465 files…\n",
      "  copied 22200/84465 files…\n",
      "  copied 22400/84465 files…\n",
      "  copied 22600/84465 files…\n",
      "  copied 22800/84465 files…\n",
      "  copied 23000/84465 files…\n",
      "  copied 23200/84465 files…\n",
      "  copied 23400/84465 files…\n",
      "  copied 23600/84465 files…\n",
      "  copied 23800/84465 files…\n",
      "  copied 24000/84465 files…\n",
      "  copied 24200/84465 files…\n",
      "  copied 24400/84465 files…\n",
      "  copied 24600/84465 files…\n",
      "  copied 24800/84465 files…\n",
      "  copied 25000/84465 files…\n",
      "  copied 25200/84465 files…\n",
      "  copied 25400/84465 files…\n",
      "  copied 25600/84465 files…\n",
      "  copied 25800/84465 files…\n",
      "  copied 26000/84465 files…\n",
      "  copied 26200/84465 files…\n",
      "  copied 26400/84465 files…\n",
      "  copied 26600/84465 files…\n",
      "  copied 26800/84465 files…\n",
      "  copied 27000/84465 files…\n",
      "  copied 27200/84465 files…\n",
      "  copied 27400/84465 files…\n",
      "  copied 27600/84465 files…\n",
      "  copied 27800/84465 files…\n",
      "  copied 28000/84465 files…\n",
      "  copied 28200/84465 files…\n",
      "  copied 28400/84465 files…\n",
      "  copied 28600/84465 files…\n",
      "  copied 28800/84465 files…\n",
      "  copied 29000/84465 files…\n",
      "  copied 29200/84465 files…\n",
      "  copied 29400/84465 files…\n",
      "  copied 29600/84465 files…\n",
      "  copied 29800/84465 files…\n",
      "  copied 30000/84465 files…\n",
      "  copied 30200/84465 files…\n",
      "  copied 30400/84465 files…\n",
      "  copied 30600/84465 files…\n",
      "  copied 30800/84465 files…\n",
      "  copied 31000/84465 files…\n",
      "  copied 31200/84465 files…\n",
      "  copied 31400/84465 files…\n",
      "  copied 31600/84465 files…\n",
      "  copied 31800/84465 files…\n",
      "  copied 32000/84465 files…\n",
      "  copied 32200/84465 files…\n",
      "  copied 32400/84465 files…\n",
      "  copied 32600/84465 files…\n",
      "  copied 32800/84465 files…\n",
      "  copied 33000/84465 files…\n",
      "  copied 33200/84465 files…\n",
      "  copied 33400/84465 files…\n",
      "  copied 33600/84465 files…\n",
      "  copied 33800/84465 files…\n",
      "  copied 34000/84465 files…\n",
      "  copied 34200/84465 files…\n",
      "  copied 34400/84465 files…\n",
      "  copied 34600/84465 files…\n",
      "  copied 34800/84465 files…\n",
      "  copied 35000/84465 files…\n",
      "  copied 35200/84465 files…\n",
      "  copied 35400/84465 files…\n",
      "  copied 35600/84465 files…\n",
      "  copied 35800/84465 files…\n",
      "  copied 36000/84465 files…\n",
      "  copied 36200/84465 files…\n",
      "  copied 36400/84465 files…\n",
      "  copied 36600/84465 files…\n",
      "  copied 36800/84465 files…\n",
      "  copied 37000/84465 files…\n",
      "  copied 37200/84465 files…\n",
      "  copied 37400/84465 files…\n",
      "  copied 37600/84465 files…\n",
      "  copied 37800/84465 files…\n",
      "  copied 38000/84465 files…\n",
      "  copied 38200/84465 files…\n",
      "  copied 38400/84465 files…\n",
      "  copied 38600/84465 files…\n",
      "  copied 38800/84465 files…\n",
      "  copied 39000/84465 files…\n",
      "  copied 39200/84465 files…\n",
      "  copied 39400/84465 files…\n",
      "  copied 39600/84465 files…\n",
      "  copied 39800/84465 files…\n",
      "  copied 40000/84465 files…\n",
      "  copied 40200/84465 files…\n",
      "  copied 40400/84465 files…\n",
      "  copied 40600/84465 files…\n",
      "  copied 40800/84465 files…\n",
      "  copied 41000/84465 files…\n",
      "  copied 41200/84465 files…\n",
      "  copied 41400/84465 files…\n",
      "  copied 41600/84465 files…\n",
      "  copied 41800/84465 files…\n",
      "  copied 42000/84465 files…\n",
      "  copied 42200/84465 files…\n",
      "  copied 42400/84465 files…\n",
      "  copied 42600/84465 files…\n",
      "  copied 42800/84465 files…\n",
      "  copied 43000/84465 files…\n",
      "  copied 43200/84465 files…\n",
      "  copied 43400/84465 files…\n",
      "  copied 43600/84465 files…\n",
      "  copied 43800/84465 files…\n",
      "  copied 44000/84465 files…\n",
      "  copied 44200/84465 files…\n",
      "  copied 44400/84465 files…\n",
      "  copied 44600/84465 files…\n",
      "  copied 44800/84465 files…\n",
      "  copied 45000/84465 files…\n",
      "  copied 45200/84465 files…\n",
      "  copied 45400/84465 files…\n",
      "  copied 45600/84465 files…\n",
      "  copied 45800/84465 files…\n",
      "  copied 46000/84465 files…\n",
      "  copied 46200/84465 files…\n",
      "  copied 46400/84465 files…\n",
      "  copied 46600/84465 files…\n",
      "  copied 46800/84465 files…\n",
      "  copied 47000/84465 files…\n",
      "  copied 47200/84465 files…\n",
      "  copied 47400/84465 files…\n",
      "  copied 47600/84465 files…\n",
      "  copied 47800/84465 files…\n",
      "  copied 48000/84465 files…\n",
      "  copied 48200/84465 files…\n",
      "  copied 48400/84465 files…\n",
      "  copied 48600/84465 files…\n",
      "  copied 48800/84465 files…\n",
      "  copied 49000/84465 files…\n",
      "  copied 49200/84465 files…\n",
      "  copied 49400/84465 files…\n",
      "  copied 49600/84465 files…\n",
      "  copied 49800/84465 files…\n",
      "  copied 50000/84465 files…\n",
      "  copied 50200/84465 files…\n",
      "  copied 50400/84465 files…\n",
      "  copied 50600/84465 files…\n",
      "  copied 50800/84465 files…\n",
      "  copied 51000/84465 files…\n",
      "  copied 51200/84465 files…\n",
      "  copied 51400/84465 files…\n",
      "  copied 51600/84465 files…\n",
      "  copied 51800/84465 files…\n",
      "  copied 52000/84465 files…\n",
      "  copied 52200/84465 files…\n",
      "  copied 52400/84465 files…\n",
      "  copied 52600/84465 files…\n",
      "  copied 52800/84465 files…\n",
      "  copied 53000/84465 files…\n",
      "  copied 53200/84465 files…\n",
      "  copied 53400/84465 files…\n",
      "  copied 53600/84465 files…\n",
      "  copied 53800/84465 files…\n",
      "  copied 54000/84465 files…\n",
      "  copied 54200/84465 files…\n",
      "  copied 54400/84465 files…\n",
      "  copied 54600/84465 files…\n",
      "  copied 54800/84465 files…\n",
      "  copied 55000/84465 files…\n",
      "  copied 55200/84465 files…\n",
      "  copied 55400/84465 files…\n",
      "  copied 55600/84465 files…\n",
      "  copied 55800/84465 files…\n",
      "  copied 56000/84465 files…\n",
      "  copied 56200/84465 files…\n",
      "  copied 56400/84465 files…\n",
      "  copied 56600/84465 files…\n",
      "  copied 56800/84465 files…\n",
      "  copied 57000/84465 files…\n",
      "  copied 57200/84465 files…\n",
      "  copied 57400/84465 files…\n",
      "  copied 57600/84465 files…\n",
      "  copied 57800/84465 files…\n",
      "  copied 58000/84465 files…\n",
      "  copied 58200/84465 files…\n",
      "  copied 58400/84465 files…\n",
      "  copied 58600/84465 files…\n",
      "  copied 58800/84465 files…\n",
      "  copied 59000/84465 files…\n",
      "  copied 59200/84465 files…\n",
      "  copied 59400/84465 files…\n",
      "  copied 59600/84465 files…\n",
      "  copied 59800/84465 files…\n",
      "  copied 60000/84465 files…\n",
      "  copied 60200/84465 files…\n",
      "  copied 60400/84465 files…\n",
      "  copied 60600/84465 files…\n",
      "  copied 60800/84465 files…\n",
      "  copied 61000/84465 files…\n",
      "  copied 61200/84465 files…\n",
      "  copied 61400/84465 files…\n",
      "  copied 61600/84465 files…\n",
      "  copied 61800/84465 files…\n",
      "  copied 62000/84465 files…\n",
      "  copied 62200/84465 files…\n",
      "  copied 62400/84465 files…\n",
      "  copied 62600/84465 files…\n",
      "  copied 62800/84465 files…\n",
      "  copied 63000/84465 files…\n",
      "  copied 63200/84465 files…\n",
      "  copied 63400/84465 files…\n",
      "  copied 63600/84465 files…\n",
      "  copied 63800/84465 files…\n",
      "  copied 64000/84465 files…\n",
      "  copied 64200/84465 files…\n",
      "  copied 64400/84465 files…\n",
      "  copied 64600/84465 files…\n",
      "  copied 64800/84465 files…\n",
      "  copied 65000/84465 files…\n",
      "  copied 65200/84465 files…\n",
      "  copied 65400/84465 files…\n",
      "  copied 65600/84465 files…\n",
      "  copied 65800/84465 files…\n",
      "  copied 66000/84465 files…\n",
      "  copied 66200/84465 files…\n",
      "  copied 66400/84465 files…\n",
      "  copied 66600/84465 files…\n",
      "  copied 66800/84465 files…\n",
      "  copied 67000/84465 files…\n",
      "  copied 67200/84465 files…\n",
      "  copied 67400/84465 files…\n",
      "  copied 67600/84465 files…\n",
      "  copied 67800/84465 files…\n",
      "  copied 68000/84465 files…\n",
      "  copied 68200/84465 files…\n",
      "  copied 68400/84465 files…\n",
      "  copied 68600/84465 files…\n",
      "  copied 68800/84465 files…\n",
      "  copied 69000/84465 files…\n",
      "  copied 69200/84465 files…\n",
      "  copied 69400/84465 files…\n",
      "  copied 69600/84465 files…\n",
      "  copied 69800/84465 files…\n",
      "  copied 70000/84465 files…\n",
      "  copied 70200/84465 files…\n",
      "  copied 70400/84465 files…\n",
      "  copied 70600/84465 files…\n",
      "  copied 70800/84465 files…\n",
      "  copied 71000/84465 files…\n",
      "  copied 71200/84465 files…\n",
      "  copied 71400/84465 files…\n",
      "  copied 71600/84465 files…\n",
      "  copied 71800/84465 files…\n",
      "  copied 72000/84465 files…\n",
      "  copied 72200/84465 files…\n",
      "  copied 72400/84465 files…\n",
      "  copied 72600/84465 files…\n",
      "  copied 72800/84465 files…\n",
      "  copied 73000/84465 files…\n",
      "  copied 73200/84465 files…\n",
      "  copied 73400/84465 files…\n",
      "  copied 73600/84465 files…\n",
      "  copied 73800/84465 files…\n",
      "  copied 74000/84465 files…\n",
      "  copied 74200/84465 files…\n",
      "  copied 74400/84465 files…\n",
      "  copied 74600/84465 files…\n",
      "  copied 74800/84465 files…\n",
      "  copied 75000/84465 files…\n",
      "  copied 75200/84465 files…\n",
      "  copied 75400/84465 files…\n",
      "  copied 75600/84465 files…\n",
      "  copied 75800/84465 files…\n",
      "  copied 76000/84465 files…\n",
      "  copied 76200/84465 files…\n",
      "  copied 76400/84465 files…\n",
      "  copied 76600/84465 files…\n",
      "  copied 76800/84465 files…\n",
      "  copied 77000/84465 files…\n",
      "  copied 77200/84465 files…\n",
      "  copied 77400/84465 files…\n",
      "  copied 77600/84465 files…\n",
      "  copied 77800/84465 files…\n",
      "  copied 78000/84465 files…\n",
      "  copied 78200/84465 files…\n",
      "  copied 78400/84465 files…\n",
      "  copied 78600/84465 files…\n",
      "  copied 78800/84465 files…\n",
      "  copied 79000/84465 files…\n",
      "  copied 79200/84465 files…\n",
      "  copied 79400/84465 files…\n",
      "  copied 79600/84465 files…\n",
      "  copied 79800/84465 files…\n",
      "  copied 80000/84465 files…\n",
      "  copied 80200/84465 files…\n",
      "  copied 80400/84465 files…\n",
      "  copied 80600/84465 files…\n",
      "  copied 80800/84465 files…\n",
      "  copied 81000/84465 files…\n",
      "  copied 81200/84465 files…\n",
      "  copied 81400/84465 files…\n",
      "  copied 81600/84465 files…\n",
      "  copied 81800/84465 files…\n",
      "  copied 82000/84465 files…\n",
      "  copied 82200/84465 files…\n",
      "  copied 82400/84465 files…\n",
      "  copied 82600/84465 files…\n",
      "  copied 82800/84465 files…\n",
      "  copied 83000/84465 files…\n",
      "  copied 83200/84465 files…\n",
      "  copied 83400/84465 files…\n",
      "  copied 83600/84465 files…\n",
      "  copied 83800/84465 files…\n",
      "  copied 84000/84465 files…\n",
      "  copied 84200/84465 files…\n",
      "  copied 84400/84465 files…\n",
      "\n",
      "✅ New dataset created with stratified 70/15/15 split.\n",
      "Output root: C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_70_15_15\n",
      " train/: 59126\n",
      " val/  : 12670\n",
      " test/ : 12669\n",
      " total : 84465\n",
      "\n",
      "Per-class breakdown:\n",
      "  CNV        total= 37451  train= 26216  val=  5618  test=  5617\n",
      "  DME        total= 11594  train=  8116  val=  1739  test=  1739\n",
      "  DRUSEN     total=  8859  train=  6201  val=  1329  test=  1329\n",
      "  NORMAL     total= 26561  train= 18593  val=  3984  test=  3984\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# =============== CONFIG ===============\n",
    "INPUT_ROOT  = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_STRATIFIED_BORDER\"\n",
    "OUTPUT_ROOT = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_70_15_15\"\n",
    "\n",
    "# Target split ratios (must sum to 1.0)\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO   = 0.15\n",
    "TEST_RATIO  = 0.15\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}  # add if needed\n",
    "CLEAN_OUTPUT = False   # CAUTION: if True, deletes OUTPUT_ROOT first\n",
    "MAX_WORKERS = 8        # threads for faster copying (I/O bound)\n",
    "# ======================================\n",
    "\n",
    "assert abs((TRAIN_RATIO + VAL_RATIO + TEST_RATIO) - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "in_root = Path(INPUT_ROOT)\n",
    "out_root = Path(OUTPUT_ROOT)\n",
    "train_out = out_root / \"train\"\n",
    "val_out   = out_root / \"val\"\n",
    "test_out  = out_root / \"test\"\n",
    "\n",
    "splits_in = [in_root / \"train\", in_root / \"val\", in_root / \"test\"]\n",
    "\n",
    "def list_images(root: Path):\n",
    "    return [p for p in root.rglob(\"*\") if p.is_file() and p.suffix.lower() in EXTS]\n",
    "\n",
    "def collect_pool_by_class(input_root: Path):\n",
    "    \"\"\"\n",
    "    Collect ALL images from input_root/{train,val,test}/<class>/...\n",
    "    Returns dict: class_name -> [Path, ...]\n",
    "    \"\"\"\n",
    "    pool = defaultdict(list)\n",
    "    for split_dir in splits_in:\n",
    "        if not split_dir.exists(): \n",
    "            continue\n",
    "        for cls_dir in [d for d in split_dir.iterdir() if d.is_dir()]:\n",
    "            imgs = list_images(cls_dir)\n",
    "            if imgs:\n",
    "                pool[cls_dir.name].extend(imgs)\n",
    "    return pool\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def safe_copy(src: Path, dst: Path):\n",
    "    \"\"\"\n",
    "    Copy src -> dst. If a file with same name exists, append numeric suffix to avoid overwrite.\n",
    "    \"\"\"\n",
    "    ensure_dir(dst.parent)\n",
    "    target = dst\n",
    "    if target.exists():\n",
    "        stem, suf = target.stem, target.suffix\n",
    "        k = 1\n",
    "        while target.exists():\n",
    "            target = target.with_name(f\"{stem}_{k}{suf}\")\n",
    "            k += 1\n",
    "    shutil.copy2(src, target)\n",
    "\n",
    "def copy_many(pairs):\n",
    "    \"\"\"\n",
    "    Copy (src, dst) pairs using threads for I/O speed.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = [ex.submit(safe_copy, s, d) for s, d in pairs]\n",
    "        for i, _ in enumerate(as_completed(futs), 1):\n",
    "            if i % 200 == 0:\n",
    "                print(f\"  copied {i}/{len(pairs)} files…\")\n",
    "\n",
    "# --------- Main procedure ----------\n",
    "if not in_root.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"INPUT_ROOT not found: {in_root}\\n\"\n",
    "        f\"If this is OneDrive, right‑click the folder in Explorer → 'Always keep on this device'.\"\n",
    "    )\n",
    "\n",
    "if out_root.exists() and CLEAN_OUTPUT:\n",
    "    print(f\"⚠️ Removing existing output: {out_root}\")\n",
    "    shutil.rmtree(out_root)\n",
    "\n",
    "# Create output dirs\n",
    "ensure_dir(train_out); ensure_dir(val_out); ensure_dir(test_out)\n",
    "\n",
    "# Build pool\n",
    "pool = collect_pool_by_class(in_root)\n",
    "if not pool:\n",
    "    raise RuntimeError(f\"No class folders or images found under {in_root}/(train|val|test).\")\n",
    "\n",
    "# Split & prepare copy lists\n",
    "summary = {}\n",
    "copy_list = []  # list of (src, dst) to copy\n",
    "\n",
    "print(\"Preparing stratified 70/15/15 split per class...\")\n",
    "for cls, files in pool.items():\n",
    "    if not files:\n",
    "        print(f\"⚠️ No images for class '{cls}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(files)\n",
    "    n_total = len(files)\n",
    "    n_train = int(round(n_total * TRAIN_RATIO))\n",
    "    n_val   = int(round(n_total * VAL_RATIO))\n",
    "    # ensure totals add up exactly\n",
    "    n_test  = n_total - n_train - n_val\n",
    "\n",
    "    train_files = files[:n_train]\n",
    "    val_files   = files[n_train:n_train+n_val]\n",
    "    test_files  = files[n_train+n_val:]\n",
    "\n",
    "    # Save intended counts\n",
    "    summary[cls] = {\"total\": n_total, \"train\": len(train_files), \"val\": len(val_files), \"test\": len(test_files)}\n",
    "\n",
    "    # Build destination pairs\n",
    "    for src in train_files:\n",
    "        dst = train_out / cls / src.name\n",
    "        copy_list.append((src, dst))\n",
    "    for src in val_files:\n",
    "        dst = val_out / cls / src.name\n",
    "        copy_list.append((src, dst))\n",
    "    for src in test_files:\n",
    "        dst = test_out / cls / src.name\n",
    "        copy_list.append((src, dst))\n",
    "\n",
    "# Execute copies\n",
    "total_to_copy = len(copy_list)\n",
    "print(f\"Copying {total_to_copy} files to:\\n  {out_root}\\nThis may take a while...\")\n",
    "copy_many(copy_list)\n",
    "\n",
    "# Report\n",
    "grand_total = sum(v[\"total\"] for v in summary.values())\n",
    "grand_train = sum(v[\"train\"] for v in summary.values())\n",
    "grand_val   = sum(v[\"val\"]   for v in summary.values())\n",
    "grand_test  = sum(v[\"test\"]  for v in summary.values())\n",
    "\n",
    "print(\"\\n✅ New dataset created with stratified 70/15/15 split.\")\n",
    "print(f\"Output root: {out_root}\")\n",
    "print(f\" train/: {grand_train}\")\n",
    "print(f\" val/  : {grand_val}\")\n",
    "print(f\" test/ : {grand_test}\")\n",
    "print(f\" total : {grand_total}\\n\")\n",
    "\n",
    "print(\"Per-class breakdown:\")\n",
    "for cls in sorted(summary.keys()):\n",
    "    s = summary[cls]\n",
    "    print(f\"  {cls:<10} total={s['total']:>6}  train={s['train']:>6}  val={s['val']:>6}  test={s['test']:>6}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9f7bb",
   "metadata": {},
   "source": [
    "# Preprocessing Step 3: Downscale to 128×128 (Efficiency Option)\n",
    "We generate a compact **128×128** version of the 70/15/15 dataset for\n",
    "lightweight models and ablations. We again use aspect-ratio-preserving\n",
    "resize with black padding and preserve the split/class structure.\n",
    "\n",
    "- Input: `OCT2017_70_15_15`\n",
    "- Output: `OCT2017_128`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c7319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84465 images. Resizing to (128, 128)…\n",
      "[██████████████████████████████] 84465/84465\n",
      "\n",
      "✅ Done.\n",
      "Processed : 84465\n",
      "Skipped   : 0 (exists, OVERWRITE=False)\n",
      "Errors    : 0\n",
      "Output to : C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_128\n",
      "Elapsed   : 841.3s  (~0.010s per image)\n",
      "\n",
      "Per split/class counts (input seen):\n",
      "  test/CNV               5617\n",
      "  test/DME               1739\n",
      "  test/DRUSEN            1329\n",
      "  test/NORMAL            3984\n",
      "  train/CNV             26216\n",
      "  train/DME              8116\n",
      "  train/DRUSEN           6201\n",
      "  train/NORMAL          18593\n",
      "  val/CNV                5618\n",
      "  val/DME                1739\n",
      "  val/DRUSEN             1329\n",
      "  val/NORMAL             3984\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# =============== CONFIG ===============\n",
    "INPUT_ROOT  = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_70_15_15\"\n",
    "OUTPUT_ROOT = r\"C:\\Users\\sheno\\OneDrive\\CODCSD201F-006-SetupFile\\Desktop\\FINAL\\dataset\\OCT2017_128\"\n",
    "\n",
    "TARGET_SIZE = (128, 128)         # (width, height)\n",
    "EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "OVERWRITE = False                 # skip files that already exist\n",
    "FORCE_GRAYSCALE = False           # set True to save as single-channel grayscale\n",
    "# =====================================\n",
    "\n",
    "def resize_with_padding(img, target_size=(128,128)):\n",
    "    \"\"\"Resize keeping aspect ratio, then pad with black to target size.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    tw, th = target_size\n",
    "    scale = min(tw / w, th / h)\n",
    "    nw, nh = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # compute padding\n",
    "    dw, dh = tw - nw, th - nh\n",
    "    top, bottom = dh // 2, dh - dh // 2\n",
    "    left, right = dw // 2, dw - dw // 2\n",
    "\n",
    "    # pad (preserve channels)\n",
    "    if resized.ndim == 2:\n",
    "        border_val = 0\n",
    "    else:\n",
    "        border_val = (0, 0, 0)\n",
    "    out = cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
    "                             borderType=cv2.BORDER_CONSTANT, value=border_val)\n",
    "    return out\n",
    "\n",
    "def iter_images(root, exts):\n",
    "    root = Path(root)\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            yield p\n",
    "\n",
    "def main():\n",
    "    in_root  = Path(INPUT_ROOT)\n",
    "    out_root = Path(OUTPUT_ROOT)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not in_root.exists():\n",
    "        print(\"❌ INPUT_ROOT does not exist:\", in_root)\n",
    "        print(\"Tip: If this is OneDrive, right-click the folder in Explorer → 'Always keep on this device'.\")\n",
    "        return\n",
    "\n",
    "    files = list(iter_images(in_root, EXTS))\n",
    "    if not files:\n",
    "        print(\"❌ No images found under:\", in_root)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(files)} images. Resizing to {TARGET_SIZE}…\")\n",
    "    t0 = time()\n",
    "    processed = skipped = errors = 0\n",
    "\n",
    "    # simple per split/class counts\n",
    "    buckets = defaultdict(int)\n",
    "\n",
    "    def progress(i, n, bar_len=30):\n",
    "        frac = (i + 1) / n\n",
    "        filled = int(bar_len * frac)\n",
    "        bar = \"█\" * filled + \"·\" * (bar_len - filled)\n",
    "        print(f\"\\r[{bar}] {i+1}/{n}\", end=\"\", flush=True)\n",
    "\n",
    "    for i, src in enumerate(files):\n",
    "        rel = src.relative_to(in_root)\n",
    "        dst = out_root / rel\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # bucket key like \"train/CNV\"\n",
    "        parts = rel.parts\n",
    "        if len(parts) >= 2 and parts[0].lower() in {\"train\",\"val\",\"test\"}:\n",
    "            bucket = f\"{parts[0]}/{parts[1]}\"\n",
    "        else:\n",
    "            bucket = \"other\"\n",
    "        buckets[bucket] += 1\n",
    "\n",
    "        if dst.exists() and not OVERWRITE:\n",
    "            skipped += 1\n",
    "            progress(i, len(files))\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(src), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            errors += 1\n",
    "            progress(i, len(files))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if FORCE_GRAYSCALE:\n",
    "                if img.ndim == 3:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            out = resize_with_padding(img, TARGET_SIZE)\n",
    "            # ensure type for saving\n",
    "            if out.dtype != 'uint8':\n",
    "                out = out.clip(0, 255).astype('uint8')\n",
    "            cv2.imwrite(str(dst), out)\n",
    "            processed += 1\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "        finally:\n",
    "            progress(i, len(files))\n",
    "\n",
    "    dt = time() - t0\n",
    "    print(\"\\n\\n✅ Done.\")\n",
    "    print(f\"Processed : {processed}\")\n",
    "    print(f\"Skipped   : {skipped} (exists, OVERWRITE={OVERWRITE})\")\n",
    "    print(f\"Errors    : {errors}\")\n",
    "    print(f\"Output to : {out_root}\")\n",
    "    print(f\"Elapsed   : {dt:.1f}s  (~{dt / max(1, processed):.3f}s per image)\\n\")\n",
    "\n",
    "    print(\"Per split/class counts (input seen):\")\n",
    "    for k in sorted(buckets.keys()):\n",
    "        print(f\"  {k:<20} {buckets[k]:>6}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
